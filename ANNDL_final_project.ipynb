{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNDL Final Project: _Jeopardy!_ Dollar Value Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import (value, question, answer) three-ples from CSV.\n",
    "data = []\n",
    "with open(\"/Users/fiordali/Downloads/JEOPARDY_CSV.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        data.append(row[4:])\n",
    "\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with all 216,931 rows from the CSV file, which we will clean up.\n",
    "clean_data = []\n",
    "\n",
    "dollar_values_map = {\"$200 \": 0, \"$400 \": 1, \"$600 \": 2, \"$800 \": 3, \"$1,000 \": 4,\n",
    "                     \"$1,200 \": 5, \"$1,600 \": 6, \"$2,000 \": 7} \n",
    "\n",
    "for row in data:\n",
    "    value = row[0]\n",
    "    # Cut out rows that are Daily Double or Final Jeopardy (imperfect checking criteria).\n",
    "    if value in dollar_values_map:\n",
    "        # Map dollar value string to corresponding 'index'.\n",
    "        row[0] = dollar_values_map[value]\n",
    "        clean_data.append(row)\n",
    "\n",
    "# We now have 182,217 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally would split data in half, but currently takes too long to run.\n",
    "train_set = clean_data[:30000]\n",
    "test_set = clean_data[30000:]\n",
    "\n",
    "# Create sets of ONLY questions (remove dollar value and answer).\n",
    "all_questions = [row[1] for row in clean_data]\n",
    "lstm_train_questions = [row[1] for row in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the characters that occur in the question text to indices.\n",
    "chars = sorted(list(set(\"\".join([row[1] for row in clean_data]))))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of longest training question by character.\n",
    "max_len = 0\n",
    "counter = 0\n",
    "\n",
    "for question in all_questions:\n",
    "    for letter in question:\n",
    "        counter += 1\n",
    "    if counter > max_len:\n",
    "        max_len = counter\n",
    "    counter = 0\n",
    "\n",
    "seqlen = max_len # Length in chars of longest question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM on questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import requests as rq\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every question we indicate if a given character is present (in x) OR what the next character is (in y).\n",
    "x = np.zeros((len(lstm_train_questions), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(lstm_train_questions), seqlen, len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, question in enumerate(lstm_train_questions):\n",
    "    # Iterate over every question in the training data.\n",
    "    # For every question, pair character t with character t+1 to provide context.\n",
    "    for t, (char_in, char_out) in enumerate(zip(question[:-1], question[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True)) # ret_seq = False because we want abstract feature vector as output\n",
    "lstm_model.add(Dense(len(chars), activation='softmax'))                            \n",
    "\n",
    "lstm_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 958s 32ms/step - loss: 0.2441 - categorical_crossentropy: 0.2441 - accuracy: 0.7715\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 950s 32ms/step - loss: 0.1818 - categorical_crossentropy: 0.1818 - accuracy: 0.9300\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 973s 32ms/step - loss: 0.1660 - categorical_crossentropy: 0.1660 - accuracy: 0.9045\n",
      "Epoch 4/20\n",
      "30000/30000 [==============================] - 980s 33ms/step - loss: 0.1585 - categorical_crossentropy: 0.1585 - accuracy: 0.5824\n",
      "Epoch 5/20\n",
      "30000/30000 [==============================] - 991s 33ms/step - loss: 0.1540 - categorical_crossentropy: 0.1540 - accuracy: 0.6493\n",
      "Epoch 6/20\n",
      "30000/30000 [==============================] - 987s 33ms/step - loss: 0.1513 - categorical_crossentropy: 0.1513 - accuracy: 0.6216\n",
      "Epoch 7/20\n",
      "30000/30000 [==============================] - 986s 33ms/step - loss: 0.1491 - categorical_crossentropy: 0.1491 - accuracy: 0.7564\n",
      "Epoch 8/20\n",
      "30000/30000 [==============================] - 1002s 33ms/step - loss: 0.1476 - categorical_crossentropy: 0.1476 - accuracy: 0.7741\n",
      "Epoch 9/20\n",
      "30000/30000 [==============================] - 989s 33ms/step - loss: 0.1464 - categorical_crossentropy: 0.1464 - accuracy: 0.6902\n",
      "Epoch 10/20\n",
      "30000/30000 [==============================] - 1015s 34ms/step - loss: 0.1454 - categorical_crossentropy: 0.1454 - accuracy: 0.7808\n",
      "Epoch 11/20\n",
      "30000/30000 [==============================] - 987s 33ms/step - loss: 0.1446 - categorical_crossentropy: 0.1446 - accuracy: 0.7407\n",
      "Epoch 12/20\n",
      "30000/30000 [==============================] - 987s 33ms/step - loss: 0.1439 - categorical_crossentropy: 0.1439 - accuracy: 0.6850\n",
      "Epoch 13/20\n",
      "30000/30000 [==============================] - 992s 33ms/step - loss: 0.1432 - categorical_crossentropy: 0.1432 - accuracy: 0.7758\n",
      "Epoch 14/20\n",
      "30000/30000 [==============================] - 990s 33ms/step - loss: 0.1427 - categorical_crossentropy: 0.1427 - accuracy: 0.5299\n",
      "Epoch 15/20\n",
      "30000/30000 [==============================] - 987s 33ms/step - loss: 0.1422 - categorical_crossentropy: 0.1422 - accuracy: 0.4521\n",
      "Epoch 16/20\n",
      "30000/30000 [==============================] - 986s 33ms/step - loss: 0.1417 - categorical_crossentropy: 0.1417 - accuracy: 0.3777\n",
      "Epoch 17/20\n",
      "30000/30000 [==============================] - 1028s 34ms/step - loss: 0.1434 - categorical_crossentropy: 0.1434 - accuracy: 0.7256\n",
      "Epoch 18/20\n",
      "30000/30000 [==============================] - 992s 33ms/step - loss: 0.1415 - categorical_crossentropy: 0.1415 - accuracy: 0.7687\n",
      "Epoch 19/20\n",
      "30000/30000 [==============================] - 989s 33ms/step - loss: 0.1410 - categorical_crossentropy: 0.1410 - accuracy: 0.8062\n",
      "Epoch 20/20\n",
      "30000/30000 [==============================] - 990s 33ms/step - loss: 0.1405 - categorical_crossentropy: 0.1405 - accuracy: 0.7310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1441843d0>"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FF on feature vectors from LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature vectors for training questions.\n",
    "# The feature vectors will be the x_train data for the FF network.\n",
    "ff_train_vectors = []\n",
    "\n",
    "for i in range(len(lstm_train_questions)):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(lstm_train_questions[i]):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    pred = lstm_model.predict(x_pred, verbose=0)\n",
    "    ff_train_vectors.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data to train FF network.\n",
    "# (Feature vectors have same index as their original question and dollar value.)\n",
    "x = array(ff_train_vectors)                 # Pass in feature vectors representing question text.\n",
    "y = array([row[0] for row in train_set])    # Expect dollar value associated with each question as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each 4d vector into a 2d vector\n",
    "# (7500, 1, 540, 126) -> (7500, 68040)\n",
    "x_train_ff = x.reshape(-1,1*seqlen*126)\n",
    "\n",
    "# Reshape each 1d digit label into 2d one-hot encoding\n",
    "y_train_ff = keras.utils.to_categorical(y, num_classes=8)     # There are 8 dollar values (mapped as 0-7)\n",
    "\n",
    "ff_model = Sequential()\n",
    "\n",
    "ff_model.add(Dense(1024, input_dim=seqlen*126, activation='relu'))\n",
    "ff_model.add(Dropout(0.5))\n",
    "ff_model.add(Dense(512, activation='sigmoid'))\n",
    "ff_model.add(Dropout(0.5))\n",
    "ff_model.add(Dense(8, activation='sigmoid'))\n",
    "\n",
    "ff_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 605s 60ms/step - loss: 0.4007 - accuracy: 0.8650\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 310s 31ms/step - loss: 0.3757 - accuracy: 0.8745\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 304s 30ms/step - loss: 0.3721 - accuracy: 0.8750\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 0.3697 - accuracy: 0.8749\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 0.3685 - accuracy: 0.8750\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 311s 31ms/step - loss: 0.3663 - accuracy: 0.8750\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 298s 30ms/step - loss: 0.3664 - accuracy: 0.8750\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 262s 26ms/step - loss: 0.3654 - accuracy: 0.8750\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 0.3644 - accuracy: 0.8750\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 331s 33ms/step - loss: 0.3635 - accuracy: 0.8750\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 285s 28ms/step - loss: 0.3625 - accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 273s 27ms/step - loss: 0.3610 - accuracy: 0.8750\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 256s 26ms/step - loss: 0.3591 - accuracy: 0.8750\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 302s 30ms/step - loss: 0.3566 - accuracy: 0.8750\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 380s 38ms/step - loss: 0.3530 - accuracy: 0.8751\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 0.3494 - accuracy: 0.8751\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 302s 30ms/step - loss: 0.3454 - accuracy: 0.8754\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 0.3408 - accuracy: 0.8758\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 269s 27ms/step - loss: 0.3340 - accuracy: 0.8771\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 298s 30ms/step - loss: 0.3280 - accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x145549350>"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model.fit(x_train_ff, y_train_ff,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04556538 0.14046079 0.04831688 0.36865437 0.16488083 0.04652775\n",
      "  0.11479472 0.19862346]]\n"
     ]
    }
   ],
   "source": [
    "x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "for t, char in enumerate(test_set[300][1]):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "lstm_pred = lstm_model.predict(x_pred, verbose=0)\n",
    "\n",
    "ff_pred = ff_model.predict(lstm_pred.reshape(-1,seqlen*126))\n",
    "\n",
    "print(ff_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the testing data for the FF network.\n",
    "x_test_ff = []  # Feature vectors.\n",
    "y_test_ff = []  # Corresponding dollar values.\n",
    "\n",
    "for i in range(10000):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(test_set[i][1]):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    pred = lstm_model.predict(x_pred, verbose=0)\n",
    "    x_test_ff.append(pred)\n",
    "    \n",
    "y_test_ff = [row[0] for row in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_value(prediction):\n",
    "    max_val = 0\n",
    "    max_idx = 0\n",
    "    for idx, item in enumerate(prediction):\n",
    "        if item > max_val:\n",
    "            max_val = item\n",
    "            max_idx = idx\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Row # 20\n",
      "test_set:\n",
      " [3, 'Scour the barnyards to find one of these percussion instruments that can be part of a drum set', 'cowbell']\n",
      "[0.09896036 0.08963361 0.14012063 0.3814013  0.32389438 0.12915163\n",
      " 0.16155075 0.15885288]\n",
      "3\n",
      "---- Row # 21\n",
      "test_set:\n",
      " [0, 'In 1958 the theme of the first national week for these places was \"Wake up & read!\"', 'libraries']\n",
      "[0.22961381 0.09698592 0.13532788 0.34733292 0.24060984 0.1315212\n",
      " 0.11048964 0.16334812]\n",
      "3\n",
      "---- Row # 22\n",
      "test_set:\n",
      " [4, 'Christopher Isherwood & Sally Bowles are characters in this 1951 play on which a 1966 musical was partly based', 'I Am A Camera (The musical was \"Cabaret\")']\n",
      "[0.08063252 0.135028   0.17084324 0.20714566 0.36876896 0.1562017\n",
      " 0.19673228 0.24317306]\n",
      "4\n",
      "---- Row # 23\n",
      "test_set:\n",
      " [3, 'Tags for luggage headed for this airport appropriately read \"LAX\"', 'Los Angeles (International)']\n",
      "[0.1995237  0.1493033  0.13776202 0.28116    0.32990602 0.1384628\n",
      " 0.13212293 0.15277597]\n",
      "4\n",
      "---- Row # 24\n",
      "test_set:\n",
      " [5, 'A Vegas casino bears the name of this character from the Tales of Scheherazade', 'Aladdin']\n",
      "[0.16726618 0.14353195 0.13538736 0.3600499  0.2712986  0.1454396\n",
      " 0.1618902  0.14048342]\n",
      "3\n",
      "---- Row # 25\n",
      "test_set:\n",
      " [2, 'The Biblical message some might get from <a href=\"http://www.j-archive.com/media/2006-05-18_J_08.jpg\" target=\"_blank\">this couple</a> is \"Move forward and don\\'t look back!\"', 'Lot & his wife']\n",
      "[0.15866394 0.32232597 0.04833747 0.39014506 0.04455132 0.02127957\n",
      " 0.01153009 0.02394271]\n",
      "3\n",
      "---- Row # 26\n",
      "test_set:\n",
      " [7, 'Elizabeth returned to London a semi-invalid & spent the next 5 years confined to her room on this street', 'Wimpole Street']\n",
      "[0.13507223 0.12814775 0.14153026 0.33801404 0.37875387 0.10050689\n",
      " 0.1279402  0.14639384]\n",
      "4\n",
      "---- Row # 27\n",
      "test_set:\n",
      " [1, 'To shine something, or an adjective meaning something from Warsaw', 'polish & Polish']\n",
      "[0.17093262 0.11954585 0.13421644 0.32714355 0.36864263 0.09227175\n",
      " 0.09993099 0.1545758 ]\n",
      "4\n",
      "---- Row # 28\n",
      "test_set:\n",
      " [1, 'Edgar succumbed to sentox gas at CTU in a 2006 episode of this show', '24']\n",
      "[0.2211661  0.14455004 0.1348969  0.22480455 0.38631564 0.11113004\n",
      " 0.14407875 0.16058844]\n",
      "4\n",
      "---- Row # 29\n",
      "test_set:\n",
      " [3, 'If Snickers & Hershey bars are your obsession, you may be this 10-letter word', 'a chocoholic']\n",
      "[0.14261125 0.14696094 0.14091076 0.20530364 0.38490036 0.13843222\n",
      " 0.18596648 0.20250225]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Quick check for a few text inputs.\n",
    "for i in range(20, 30):\n",
    "    prediction = ff_model.predict((x_test_ff[i]).reshape(-1,seqlen*126))\n",
    "    print(\"---- Row #\", i)\n",
    "    print(\"test_set:\\n\", test_set[i])\n",
    "    print(prediction[0])\n",
    "    print(pred_to_value(prediction[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Emma Boettcher Thesis](https://futurism.com/jeopardy-emma-boettcher-ai-james-holzhauer)\n",
    "* [A Gentle Introduction to LSTM Autoencoders](https://machinelearningmastery.com/lstm-autoencoders/)\n",
    "* [LSTM â€“ nuggest for practical application](https://towardsdatascience.com/lstm-nuggets-for-practical-applications-5beef5252092)\n",
    "* [Understanding Stateful LSTM RNNs Python Keras](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\n",
    "* [Reshape Input Data LSTMs](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/)\n",
    "* [How to use return_state](https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/)\n",
    "* [One-hot Encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)\n",
    "* [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
    "* [ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "* [First Neural Network Project](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
