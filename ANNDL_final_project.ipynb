{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNDL Final Project: _Jeopardy!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import (value, question, answer) three-ples from CSV.\n",
    "data = []\n",
    "with open(\"/Users/fiordali/Downloads/JEOPARDY_CSV.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        data.append(row[4:])\n",
    "\n",
    "random.shuffle(data) # Do I have to avoid shuffling the data/recreating the train and test data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with all 216,931 rows from the CSV file, which we will clean up.\n",
    "clean_data = []\n",
    "\n",
    "# TODO: Use sklearn labelencoder instead\n",
    "dollar_values_map = {\"$200 \": 0, \"$400 \": 1, \"$600 \": 2, \"$800 \": 3, \"$1,000 \": 4,\n",
    "                     \"$1,200 \": 5, \"$1,600 \": 6, \"$2,000 \": 7} \n",
    "\n",
    "for row in data:\n",
    "    # Cut out rows that are Daily Double or Final Jeopardy (imperfect checking criteria)\n",
    "    value = row[0]\n",
    "    if value in dollar_values_map:\n",
    "        # Map dollar value string to corresponding 'index'.\n",
    "        row[0] = dollar_values_map[value]\n",
    "        clean_data.append(row)\n",
    "\n",
    "# We now have 182,217 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into two randomized groups: testing and training data.\n",
    "idx = len(clean_data) // 2\n",
    "\n",
    "# Ideally would split data in half, but currently takes too long to run.\n",
    "train_set = clean_data[:10000]\n",
    "test_set = clean_data[10000:20000]\n",
    "\n",
    "# Create sets of ONLY questions (remove dollar value and answer).\n",
    "all_questions = [row[1] for row in clean_data]\n",
    "lstm_train_questions = [row[1] for row in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the characters that occur in the question text to indices.\n",
    "chars = sorted(list(set(\"\".join([row[1] for row in clean_data]))))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of longest training question by character.\n",
    "max_len = 0\n",
    "counter = 0\n",
    "\n",
    "for question in all_questions:\n",
    "    for letter in question:\n",
    "        counter += 1\n",
    "    if counter > max_len:\n",
    "        max_len = counter\n",
    "    counter = 0\n",
    "\n",
    "seqlen = max_len # Length in chars of longest question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM on questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import requests as rq\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every question we indicate if a given character is present (in x) OR what the next character is (in y).\n",
    "x = np.zeros((len(lstm_train_questions), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(lstm_train_questions), seqlen, len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, question in enumerate(lstm_train_questions):\n",
    "    # Iterate over every question in the training data.\n",
    "    # For every question, pair character t with character t+1 to provide context.\n",
    "    for t, (char_in, char_out) in enumerate(zip(question[:-1], question[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True)) # ret_seq = False because we want abstract feature vector as output\n",
    "lstm_model.add(Dense(len(chars), activation='softmax'))                            # CUT THIS LAYER? Or is this the feature vector we pass to FF?\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 0.3031 - categorical_crossentropy: 0.3031 - accuracy: 0.7359\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 336s 34ms/step - loss: 0.2327 - categorical_crossentropy: 0.2327 - accuracy: 0.5989\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 337s 34ms/step - loss: 0.2042 - categorical_crossentropy: 0.2042 - accuracy: 0.5221\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 340s 34ms/step - loss: 0.1904 - categorical_crossentropy: 0.1904 - accuracy: 0.4087\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 354s 35ms/step - loss: 0.1808 - categorical_crossentropy: 0.1808 - accuracy: 0.4626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x148602090>"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FF on feature vectors from LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 860, 126)\n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "[[[1.9444054e-02 4.5492626e-03 7.1101622e-03 ... 2.8026458e-03\n",
      "   3.0922021e-03 2.1714736e-03]\n",
      "  [5.3738710e-02 2.2899786e-03 4.9155415e-03 ... 3.6330149e-04\n",
      "   6.3680724e-04 3.2492765e-04]\n",
      "  [1.2622854e-01 6.4242259e-04 3.5330197e-03 ... 1.4588161e-05\n",
      "   5.9491005e-05 1.1841793e-05]\n",
      "  ...\n",
      "  [1.5930353e-01 8.8263291e-04 2.0451769e-02 ... 8.9476976e-05\n",
      "   4.2055608e-04 2.7638655e-05]\n",
      "  [1.9290237e-01 1.5005581e-03 5.2973803e-02 ... 7.2356008e-05\n",
      "   4.5601735e-04 2.6879738e-05]\n",
      "  [1.9271864e-01 2.3862971e-03 8.8099293e-02 ... 4.8293325e-05\n",
      "   4.1505072e-04 2.1401869e-05]]]\n"
     ]
    }
   ],
   "source": [
    "# A quick way to check predictions. DELETE LATER.\n",
    "x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "print(x_pred.shape)\n",
    "pred = lstm_model.predict(x_pred, verbose=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature vectors for training questions.\n",
    "# The feature vectors will be the x_train data for the FF network.\n",
    "ff_train_vectors = []\n",
    "\n",
    "for i in range(len(lstm_train_questions)):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(lstm_train_questions[i]):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    pred = lstm_model.predict(x_pred, verbose=0)\n",
    "    ff_train_vectors.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data to train FF network.\n",
    "# (Feature vectors have same index as their original question and dollar value.)\n",
    "x = array(ff_train_vectors)                 # Pass in feature vectors representing question text.\n",
    "y = array([row[0] for row in train_set])    # Expect dollar value associated with each question as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000 10000\n",
      "10000\n",
      "0\n",
      "(10000, 1, 860, 126)\n"
     ]
    }
   ],
   "source": [
    "print(len(ff_train_vectors))\n",
    "print(len(x), len(y))\n",
    "print(len(lstm_train_questions))\n",
    "print(y[1])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each 4d vector into a 2d vector\n",
    "# (7500, 1, 540, 126) -> (7500, 68040)\n",
    "x_train_ff = x.reshape(-1,1*seqlen*126)\n",
    "\n",
    "# Reshape each 1d digit label into 2d one-hot encoding\n",
    "y_train_ff = keras.utils.to_categorical(y, num_classes=8)     # There are 8 dollar values (mapped as 0-7)\n",
    "\n",
    "ff_model = Sequential()\n",
    "\n",
    "# NO ACTIVATIONS IN OUTPUT (NO PREDICTION).\n",
    "# ff_model.add(Dense(512, input_dim=540*126, activation='relu'))\n",
    "# ff_model.add(Dropout(0.5))\n",
    "# ff_model.add(Dense(256, activation='relu'))\n",
    "# ff_model.add(Dropout(0.5))\n",
    "# ff_model.add(Dense(8, activation='relu'))\n",
    "\n",
    "# ff_model.add(Dense(512, input_dim=540*126, activation='relu'))\n",
    "# ff_model.add(Dropout(0.2))\n",
    "# ff_model.add(Dense(8, activation='relu'))\n",
    "# ff_model.add(Dropout(0.2))\n",
    "\n",
    "# PREDICTION VECTOR FILLED WITH ACTIVATIONS!!! SOME (4/10) ACCURATE!\n",
    "ff_model.add(Dense(1024, input_dim=seqlen*126, activation='relu'))\n",
    "ff_model.add(Dropout(0.5))\n",
    "ff_model.add(Dense(512, activation='sigmoid'))\n",
    "ff_model.add(Dropout(0.5))\n",
    "ff_model.add(Dense(8, activation='sigmoid'))\n",
    "\n",
    "ff_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 605s 60ms/step - loss: 0.4007 - accuracy: 0.8650\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 310s 31ms/step - loss: 0.3757 - accuracy: 0.8745\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 304s 30ms/step - loss: 0.3721 - accuracy: 0.8750\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 0.3697 - accuracy: 0.8749\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 0.3685 - accuracy: 0.8750\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 311s 31ms/step - loss: 0.3663 - accuracy: 0.8750\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 298s 30ms/step - loss: 0.3664 - accuracy: 0.8750\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 262s 26ms/step - loss: 0.3654 - accuracy: 0.8750\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 329s 33ms/step - loss: 0.3644 - accuracy: 0.8750\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 331s 33ms/step - loss: 0.3635 - accuracy: 0.8750\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 285s 28ms/step - loss: 0.3625 - accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 273s 27ms/step - loss: 0.3610 - accuracy: 0.8750\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 256s 26ms/step - loss: 0.3591 - accuracy: 0.8750\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 302s 30ms/step - loss: 0.3566 - accuracy: 0.8750\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 380s 38ms/step - loss: 0.3530 - accuracy: 0.8751\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 332s 33ms/step - loss: 0.3494 - accuracy: 0.8751\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 302s 30ms/step - loss: 0.3454 - accuracy: 0.8754\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 289s 29ms/step - loss: 0.3408 - accuracy: 0.8758\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 269s 27ms/step - loss: 0.3340 - accuracy: 0.8771\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 298s 30ms/step - loss: 0.3280 - accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x145549350>"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model.fit(x_train_ff, y_train_ff,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04556538 0.14046079 0.04831688 0.36865437 0.16488083 0.04652775\n",
      "  0.11479472 0.19862346]]\n"
     ]
    }
   ],
   "source": [
    "x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "for t, char in enumerate(test_set[300][1]):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "lstm_pred = lstm_model.predict(x_pred, verbose=0)\n",
    "\n",
    "ff_pred = ff_model.predict(lstm_pred.reshape(-1,seqlen*126))\n",
    "\n",
    "print(ff_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the testing data for the FF network.\n",
    "x_test_ff = []  # Feature vectors.\n",
    "y_test_ff = []  # Corresponding dollar values.\n",
    "\n",
    "for i in range(10000):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(test_set[i][1]):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    pred = lstm_model.predict(x_pred, verbose=0)\n",
    "    x_test_ff.append(pred)\n",
    "    \n",
    "y_test_ff = [row[0] for row in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test_ff), len(y_test_ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_value(prediction):\n",
    "    max_val = 0\n",
    "    max_idx = 0\n",
    "    for idx, item in enumerate(prediction):\n",
    "        if item > max_val:\n",
    "            max_val = item\n",
    "            max_idx = idx\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Row # 10\n",
      "test_set:\n",
      " [6, '\"Coffey\\'s Hands\" was the third installment of this novel that was published in serial form in 1996', 'The Green Mile']\n",
      "[0.17344937 0.14682752 0.16689369 0.13340704 0.20009877 0.0929205\n",
      " 0.11024223 0.09477271]\n",
      "4\n",
      "---- Row # 11\n",
      "test_set:\n",
      " [0, '\"Towards thee I roll, thou all-destroying but unconquering whale\"', 'Moby Dick']\n",
      "[0.5369545  0.16861244 0.08380336 0.17918646 0.06361631 0.04366046\n",
      " 0.02967537 0.05044321]\n",
      "0\n",
      "---- Row # 12\n",
      "test_set:\n",
      " [0, 'Familiar shape of the Jefferson National Expansion Memorial\\'s \"Gateway\"', 'Arch']\n",
      "[0.20243259 0.29951757 0.06953414 0.24006778 0.04568674 0.04832096\n",
      " 0.07424676 0.10022949]\n",
      "1\n",
      "---- Row # 13\n",
      "test_set:\n",
      " [1, 'In 1962 he \"return\"ed to West Point to give an emotional speech on \"Duty, Honor, Country\"', 'Douglas MacArthur']\n",
      "[0.12418361 0.209118   0.15708816 0.3069174  0.12163036 0.0150142\n",
      " 0.02727487 0.09324985]\n",
      "3\n",
      "---- Row # 14\n",
      "test_set:\n",
      " [0, 'This African river enters the Mediterranean through two main branches:  the Damietta & the Rosetta', 'The Nile']\n",
      "[0.08352834 0.18351258 0.02728338 0.17478414 0.1190768  0.0761947\n",
      " 0.17412728 0.31759414]\n",
      "7\n",
      "---- Row # 15\n",
      "test_set:\n",
      " [4, 'This brother of Napoleon was named king of Spain in 1808 after Napoleon conquered the country', 'Joseph Bonaparte']\n",
      "[0.26317334 0.14211154 0.1809193  0.25335523 0.12085897 0.06284507\n",
      " 0.11854288 0.11123718]\n",
      "0\n",
      "---- Row # 16\n",
      "test_set:\n",
      " [4, 'The Papal Court, officially', 'The Holy See']\n",
      "[0.1846075  0.1693133  0.07659654 0.20249136 0.09241932 0.10604101\n",
      " 0.1624692  0.16523813]\n",
      "3\n",
      "---- Row # 17\n",
      "test_set:\n",
      " [7, '\"A Thousand Splendid Suns\", Khaled Hosseini\\'s follow-up to \"The Kite Runner\", is also mainly set in this country', 'Afghanistan']\n",
      "[0.09503985 0.2866528  0.10959351 0.14793682 0.27721345 0.04504367\n",
      " 0.09040578 0.15696716]\n",
      "1\n",
      "---- Row # 18\n",
      "test_set:\n",
      " [0, \"When checking this at kitty's femoral artery, normally it should be from about 160 to 220\", 'the pulse']\n",
      "[0.19482185 0.31271407 0.07233097 0.16268228 0.11438458 0.07201081\n",
      " 0.04312696 0.1590674 ]\n",
      "1\n",
      "---- Row # 19\n",
      "test_set:\n",
      " [4, \"Let the kids cool off by watching an ice show at Silverwood Theme Park near Coeur d'Alene in this state\", 'Idaho']\n",
      "[0.16193675 0.17183185 0.12199164 0.16360433 0.258826   0.04165153\n",
      " 0.13715966 0.1750159 ]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    prediction = ff_model.predict((x_test_ff[i]).reshape(-1,seqlen*126))\n",
    "    print(\"---- Row #\", i)\n",
    "    print(\"test_set:\\n\", test_set[i])\n",
    "    print(prediction[0])\n",
    "    print(pred_to_value(prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = ff_model.evaluate(array(x_test_ff).reshape(-1,540*126), y_test_ff[0], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Emma Boettcher Thesis](https://futurism.com/jeopardy-emma-boettcher-ai-james-holzhauer)\n",
    "* [A Gentle Introduction to LSTM Autoencoders](https://machinelearningmastery.com/lstm-autoencoders/)\n",
    "* [LSTM – nuggest for practical application](https://towardsdatascience.com/lstm-nuggets-for-practical-applications-5beef5252092)\n",
    "* [Understanding Stateful LSTM RNNs Python Keras](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\n",
    "* [Reshape Input Data LSTMs](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/)\n",
    "* [How to use return_state](https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/)\n",
    "* [One-hot Encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)\n",
    "* [Dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
    "* [ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)\n",
    "* [First Neural Network Project](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
