{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNDL Final Project: _Jeopardy!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import (value, question, answer) three-ples from CSV.\n",
    "data = []\n",
    "with open(\"/Users/fiordali/Downloads/JEOPARDY_CSV.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        data.append(row[4:])\n",
    "\n",
    "random.shuffle(data) # Do I have to avoid shuffling the data/recreating the train and test data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with all 216,931 rows from the CSV file, which we will clean up.\n",
    "clean_data = []\n",
    "set_dollar_values = {\"$200\", \"$400\", \"$600\", \"$800\", \"$1000\",\n",
    "                     \"$1200\", \"$1600\", \"$2000\"}\n",
    "\n",
    "for item in data:\n",
    "    # Cut out rows that are Daily Double or Final Jeopardy (imperfect checking criteria)\n",
    "    if item[0] in set_dollar_values:\n",
    "        # Convert dollar value from strings into ints.\n",
    "        dollar_value = item[0][1:].replace(',', '')\n",
    "        item[0] = int(dollar_value)\n",
    "        clean_data.append(item)\n",
    "\n",
    "# We now have 177,850 rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into two randomized groups: testing and training data.\n",
    "idx = len(clean_data) // 2\n",
    "\n",
    "# Ideally would train on half the data points, but currently takes too long to run.\n",
    "train_set = clean_data[:7500]\n",
    "test_set = clean_data[7500:15000]\n",
    "\n",
    "# Create sets of ONLY questions (remove dollar value and answer).\n",
    "all_questions = [row[1] for row in clean_data]\n",
    "lstm_train_questions = [row[1] for row in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the characters that occur in the question text to indices.\n",
    "chars = sorted(list(set(\"\".join([row[1] for row in clean_data]))))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of longest training question by character.\n",
    "max_len = 0\n",
    "counter = 0\n",
    "\n",
    "for question in lstm_train_questions:\n",
    "    for letter in question:\n",
    "        counter += 1\n",
    "    if counter > max_len:\n",
    "        max_len = counter\n",
    "    counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM on questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import io\n",
    "import requests as rq\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = max_len # Length in chars of longest question\n",
    "\n",
    "# For every question we indicate if a given character is present (in x) OR what the next character is (in y).\n",
    "x = np.zeros((len(lstm_train_questions), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(lstm_train_questions), seqlen, len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, question in enumerate(train_questions):\n",
    "    # Iterate over every question in the training data.\n",
    "    # For every question, pair character t with character t+1 to provide context.\n",
    "    for t, (char_in, char_out) in enumerate(zip(question[:-1], question[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True)) # ret_seq = False because we want abstract feature vector as output\n",
    "lstm_model.add(Dense(len(chars), activation='softmax'))                            # CUT THIS LAYER? Or is this the feature vector we pass to FF?\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7500/7500 [==============================] - 230s 31ms/step - loss: 0.4520 - categorical_crossentropy: 0.4520 - accuracy: 0.7363\n",
      "Epoch 2/5\n",
      "7500/7500 [==============================] - 203s 27ms/step - loss: 0.3540 - categorical_crossentropy: 0.3540 - accuracy: 0.7412\n",
      "Epoch 3/5\n",
      "7500/7500 [==============================] - 197s 26ms/step - loss: 0.3111 - categorical_crossentropy: 0.3111 - accuracy: 0.8079\n",
      "Epoch 4/5\n",
      "7500/7500 [==============================] - 219s 29ms/step - loss: 0.2909 - categorical_crossentropy: 0.2909 - accuracy: 0.7988\n",
      "Epoch 5/5\n",
      "7500/7500 [==============================] - 208s 28ms/step - loss: 0.2766 - categorical_crossentropy: 0.2766 - accuracy: 0.7849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17dfb8dd0>"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FF on feature vectors from LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 593, 126)\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "[[[1.92560274e-02 4.94443811e-03 1.18119186e-02 ... 3.70258465e-03\n",
      "   3.58833931e-03 1.94786105e-03]\n",
      "  [3.90143767e-02 2.18068389e-03 9.64256283e-03 ... 5.96941100e-04\n",
      "   8.77289276e-04 3.09866242e-04]\n",
      "  [1.12579286e-01 8.13048624e-04 5.98742161e-03 ... 4.60159281e-05\n",
      "   1.26353349e-04 2.08869387e-05]\n",
      "  ...\n",
      "  [1.73682585e-01 1.01183204e-03 1.30472481e-02 ... 4.52430977e-05\n",
      "   1.37020834e-04 1.10665205e-05]\n",
      "  [1.73682585e-01 1.01183157e-03 1.30472481e-02 ... 4.52431450e-05\n",
      "   1.37020834e-04 1.10665305e-05]\n",
      "  [1.73682585e-01 1.01183204e-03 1.30472481e-02 ... 4.52431450e-05\n",
      "   1.37020834e-04 1.10665196e-05]]]\n"
     ]
    }
   ],
   "source": [
    "# A quick way to test predictions. DELETE LATER.\n",
    "x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "print(x_pred.shape)\n",
    "pred = lstm_model.predict(x_pred, verbose=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature vectors for training questions.\n",
    "# The feature vectors will be the x_train data for the FF network.\n",
    "ff_train_vectors = []\n",
    "\n",
    "for i in range(len(lstm_train_questions)):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(lstm_train_questions[i]):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    pred = lstm_model.predict(x_pred, verbose=0)\n",
    "    ff_train_vectors.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.1266250e-02 3.2636736e-04 1.0913979e-03 ... 2.2489514e-05\n",
      "   2.4427823e-04 1.3034280e-05]\n",
      "  [5.6462851e-03 1.9178340e-04 4.1151870e-04 ... 9.2137880e-06\n",
      "   2.4401390e-05 5.3854428e-06]\n",
      "  [3.7833758e-02 3.6581009e-04 2.5705697e-03 ... 1.4364036e-05\n",
      "   2.7630402e-05 3.5621633e-06]\n",
      "  ...\n",
      "  [1.7368338e-01 1.0118251e-03 1.3046853e-02 ... 4.5242617e-05\n",
      "   1.3701910e-04 1.1066317e-05]\n",
      "  [1.7368335e-01 1.0118249e-03 1.3046857e-02 ... 4.5242610e-05\n",
      "   1.3701909e-04 1.1066325e-05]\n",
      "  [1.7368330e-01 1.0118247e-03 1.3046867e-02 ... 4.5242599e-05\n",
      "   1.3701904e-04 1.1066322e-05]]]\n"
     ]
    }
   ],
   "source": [
    "print(ff_train_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data to train FF network.\n",
    "# (Feature vectors have same index as their original question and dollar value.)\n",
    "x = array(ff_train_vectors)                        # Pass in feature vectors representing question text.\n",
    "# y = array([row[0] for row in clean_data[:7500]])   # Expect dollar value associated with each question as output.\n",
    "y = array([row[0] for row in train_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "7500 7500\n",
      "7500\n",
      "400\n",
      "(7500, 1, 593, 126)\n"
     ]
    }
   ],
   "source": [
    "print(len(ff_train_vectors))\n",
    "print(len(x), len(y))\n",
    "print(len(lstm_train_questions))\n",
    "print(y[0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "# (x_mnist, y_mnist), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape each image to 1dim vector. Effectively, reshape\n",
    "# `x_train` from (60000, 28, 28) to (60000, 28*28)\n",
    "# x_train_mnist = x_mnist[:50000].reshape(-1, 28*28)\n",
    "# x_test_mnist = x_mnist[50000:].reshape(-1, 28*28)\n",
    "\n",
    "# Reshape each 4d vector into a 2d vector\n",
    "# (7500, 1, 593, 95) -> (7500, 56335)\n",
    "x_train_ff = x.reshape(-1,1*593*126)\n",
    "# print(x_train_ff.shape)\n",
    "\n",
    "# Convert y_train from vector of labels to one-hot encoding vector\n",
    "# y_mnist = keras.utils.to_categorical(y_mnist, num_classes=10)\n",
    "# y_train_mnist = y_mnist[:50000]\n",
    "# y_test_mnist = y_mnist[50000:]\n",
    "\n",
    "y_train_ff = keras.utils.to_categorical(y, num_classes=2001)     # There are 10 question values ($200-$2000)\n",
    "# print(y_train_ff)\n",
    "\n",
    "ff_model = Sequential()\n",
    "ff_model.add(Dense(512, input_dim=593*126))\n",
    "ff_model.add(Activation('relu'))\n",
    "ff_model.add(Dropout(0.2))\n",
    "ff_model.add(Dense(2001))\n",
    "ff_model.add(Activation('relu'))\n",
    "ff_model.add(Dropout(0.2))\n",
    "\n",
    "ff_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7500/7500 [==============================] - 121s 16ms/step - loss: 0.0192 - accuracy: 0.9978\n",
      "Epoch 2/20\n",
      "7500/7500 [==============================] - 58s 8ms/step - loss: 0.0148 - accuracy: 0.9989\n",
      "Epoch 3/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0150 - accuracy: 0.9989\n",
      "Epoch 4/20\n",
      "7500/7500 [==============================] - 45s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 5/20\n",
      "7500/7500 [==============================] - 47s 6ms/step - loss: 0.0150 - accuracy: 0.9989\n",
      "Epoch 6/20\n",
      "7500/7500 [==============================] - 46s 6ms/step - loss: 0.0150 - accuracy: 0.9989\n",
      "Epoch 7/20\n",
      "7500/7500 [==============================] - 43s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 8/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0150 - accuracy: 0.9989\n",
      "Epoch 9/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0148 - accuracy: 0.9989\n",
      "Epoch 10/20\n",
      "7500/7500 [==============================] - 43s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 11/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 12/20\n",
      "7500/7500 [==============================] - 43s 6ms/step - loss: 0.0150 - accuracy: 0.9989\n",
      "Epoch 13/20\n",
      "7500/7500 [==============================] - 45s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 14/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 15/20\n",
      "7500/7500 [==============================] - 45s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 16/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 17/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 18/20\n",
      "7500/7500 [==============================] - 44s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n",
      "Epoch 19/20\n",
      "7500/7500 [==============================] - 47s 6ms/step - loss: 0.0148 - accuracy: 0.9989\n",
      "Epoch 20/20\n",
      "7500/7500 [==============================] - 49s 6ms/step - loss: 0.0149 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17f0ae250>"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model.fit(x_train_ff, y_train_ff,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the testing data for the FF network.\n",
    "x_test_ff = []  # Feature vectors.\n",
    "y_test_ff = []  # Corresponding dollar values.\n",
    "\n",
    "for i in range(7500):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(test_set[i][1]):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    pred = lstm_model.predict(x_pred, verbose=0)\n",
    "    x_test_ff.append(pred)\n",
    "    \n",
    "y_test_ff = [row[0] for row in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 'Burgess Meredith as this odd bird', 'the Penguin']\n",
      "2001\n",
      "400\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "prediction = ff_model.predict((x_test_ff[0]).reshape(-1,593*126))\n",
    "print(test_set[0])\n",
    "print(len(prediction[0]))\n",
    "\n",
    "for i, item in enumerate(prediction[0]):\n",
    "    if item != 0:\n",
    "        print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_ff, y_test_ff, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Emma Boettcher Thesis](https://futurism.com/jeopardy-emma-boettcher-ai-james-holzhauer)\n",
    "* [A Gentle Introduction to LSTM Autoencoders](https://machinelearningmastery.com/lstm-autoencoders/)\n",
    "* [LSTM â€“ nuggest for practical application](https://towardsdatascience.com/lstm-nuggets-for-practical-applications-5beef5252092)\n",
    "* [Understanding Stateful LSTM RNNs Python Keras](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\n",
    "* [Reshape Input Data LSTMs](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/)\n",
    "* [How to use return_state](https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
